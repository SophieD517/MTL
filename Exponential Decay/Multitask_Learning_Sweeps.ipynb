{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlu0Srtl1uTq",
        "outputId": "a598e1b7-fb6e-4beb-ba62-5847b4cf9ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import math as m\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrzVcHbA7YmN",
        "outputId": "8920c00c-2c35-4910-bac2-90317002fd22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.14)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.9)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msophied\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220414_135644-wa4exh8k</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/sophied/multitask/runs/wa4exh8k\" target=\"_blank\">quiet-sweep-1</a></strong> to <a href=\"https://wandb.ai/sophied/multitask\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/sophied/multitask/sweeps/grvd3po5\" target=\"_blank\">https://wandb.ai/sophied/multitask/sweeps/grvd3po5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'method': 'random',\n",
            " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
            " 'parameters': {'ae_layer': {'distribution': 'int_uniform',\n",
            "                             'max': 20,\n",
            "                             'min': 7},\n",
            "                'batch_size': {'distribution': 'int_uniform',\n",
            "                               'max': 500,\n",
            "                               'min': 50},\n",
            "                'branch1_layer': {'distribution': 'int_uniform',\n",
            "                                  'max': 10,\n",
            "                                  'min': 4},\n",
            "                'latent_dim': {'distribution': 'int_uniform',\n",
            "                               'max': 10,\n",
            "                               'min': 1},\n",
            "                'learning_rate': {'distribution': 'uniform',\n",
            "                                  'max': 0.005,\n",
            "                                  'min': 0.0001},\n",
            "                'loss_scalar': {'distribution': 'uniform', 'max': 3, 'min': 1}}}\n",
            "Create sweep with ID: k3hiuw5i\n",
            "Sweep URL: https://wandb.ai/sophied/multitask/sweeps/k3hiuw5i\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "wandb.init(project=\"multitask\", entity=\"sophied\")\n",
        "sweep_config = {'method': 'random'}\n",
        "metric = {'name': 'loss', 'goal': 'minimize'   }\n",
        "sweep_config['metric'] = metric\n",
        "parameters_dict = {\n",
        "    # 'optimizer': {\n",
        "    #     'values': ['adam', 'sgd']\n",
        "    #     },\n",
        "    'latent_dim': {\n",
        "        'min': 1,\n",
        "        'max': 10,\n",
        "        'distribution': 'int_uniform'\n",
        "        # 'values': [1, 2, 3, 4, 5, 7, 10, 20]\n",
        "    },\n",
        "    'loss_scalar': {\n",
        "        'min': 1,\n",
        "        'max': 3,\n",
        "        'distribution': 'uniform'\n",
        "        # 'values': [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3]\n",
        "    },\n",
        "    'learning_rate': {\n",
        "        'min': 0.0001,\n",
        "        'max': 0.005,\n",
        "        'distribution': 'uniform'\n",
        "        # 'values': [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01]\n",
        "    },\n",
        "    'batch_size': {\n",
        "        'min': 50,\n",
        "        'max': 500,\n",
        "        'distribution': 'int_uniform'\n",
        "        # 'values': [64, 100, 128, 200, 256, 300, 500]\n",
        "    },\n",
        "    'ae_layer': {\n",
        "        'min': 7,\n",
        "        'max': 20,\n",
        "        'distribution': 'int_uniform'\n",
        "    },\n",
        "    'branch1_layer': {\n",
        "        'min': 4,\n",
        "        'max': 10,\n",
        "        'distribution': 'int_uniform'\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "import pprint\n",
        "pprint.pprint(sweep_config)\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"multitask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq72W2Bwhiza"
      },
      "outputs": [],
      "source": [
        "# https://github.com/hcarlens/pytorch-tabular/blob/master/fast_tensor_data_loader.py\n",
        "class FastTensorDataLoader:\n",
        "    \"\"\"\n",
        "    A DataLoader-like object for a set of tensors that can be much faster than\n",
        "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
        "    the dataset and calls cat (slow).\n",
        "    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n",
        "    \"\"\"\n",
        "    def __init__(self, *tensors, batch_size=32, shuffle=False):\n",
        "        \"\"\"\n",
        "        Initialize a FastTensorDataLoader.\n",
        "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
        "        :param batch_size: batch size to load.\n",
        "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
        "            iterator is created out of this object.\n",
        "        :returns: A FastTensorDataLoader.\n",
        "        \"\"\"\n",
        "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
        "        self.tensors = tensors\n",
        "\n",
        "        self.dataset_len = self.tensors[0].shape[0]\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Calculate # batches\n",
        "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
        "        if remainder > 0:\n",
        "            n_batches += 1\n",
        "        self.n_batches = n_batches\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            r = torch.randperm(self.dataset_len)\n",
        "            self.tensors = [t[r] for t in self.tensors]\n",
        "        self.i = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.i >= self.dataset_len:\n",
        "            raise StopIteration\n",
        "        batch = tuple(t[self.i:self.i+self.batch_size] for t in self.tensors)\n",
        "        self.i += self.batch_size\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "bAiIYOx92Kdw",
        "outputId": "56f187a6-1621-4f82-8dcd-ab51d1c7a17b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c7ceda9fd737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch1_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     self.encoder = torch.nn.Sequential(\n\u001b[1;32m      5\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "class Bear(torch.nn.Module):\n",
        "  def __init__(self, dim, ae_layer, branch1_layer):\n",
        "    super().__init__()\n",
        "    self.encoder = torch.nn.Sequential(\n",
        "      torch.nn.Linear(100, ae_layer),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(ae_layer, dim)\n",
        "    )\n",
        "    self.decoder = torch.nn.Sequential( \n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(dim, ae_layer),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(ae_layer, 100)\n",
        "    )\n",
        "    self.branch1 = torch.nn.Sequential(\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(dim, branch1_layer),\n",
        "      torch.nn.ReLU(),\n",
        "      torch.nn.Linear(branch1_layer, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    latent_space = self.encoder(x)\n",
        "    return latent_space, self.decoder(latent_space), self.branch1(latent_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WkvqpFNbatWU"
      },
      "outputs": [],
      "source": [
        "train_data = torch.tensor(pd.read_csv('/content/drive/MyDrive/Project_MTL/data/SyntheticData/train.csv').drop(columns='Unnamed: 0').values, requires_grad=True, dtype=torch.float32)\n",
        "train = pd.read_csv('/content/drive/MyDrive/Project_MTL/data/SyntheticData/train.csv').drop(columns='Unnamed: 0')\n",
        "validate = torch.tensor(pd.read_csv('/content/drive/MyDrive/Project_MTL/data/SyntheticData/validate.csv').drop(columns='Unnamed: 0').values, requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "X = train_data[:, :100]\n",
        "Y = train_data[:, 100:].mul(0.001)\n",
        "X_v = validate[:, :100]\n",
        "Y_v = validate[:, 100:].mul(0.001)\n",
        "\n",
        "test = pd.read_csv('/content/drive/MyDrive/Project_MTL/data/SyntheticData/test.csv').drop(columns='Unnamed: 0')#.sample(n=100)\n",
        "X_test = torch.tensor(test.drop(columns=['half_life']).values, requires_grad=True, dtype=torch.float32).to(torch.float32)\n",
        "Y_test = torch.tensor(test['half_life'].to_numpy(), requires_grad=True, dtype=torch.float32).unsqueeze(dim=1).mul(0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3ucI_zoi3AC7"
      },
      "outputs": [],
      "source": [
        "loss_func = torch.nn.MSELoss()\n",
        "def train(epochs=25, batch_size=200, config=None, learning_rate=0.005): #get train and use one dataloader\n",
        "  with wandb.init(config=config):\n",
        "    config = wandb.config\n",
        "    model = Bear(config.latent_dim, config.ae_layer, config.branch1_layer)\n",
        "    batch_size = config.batch_size\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "    train_batches = FastTensorDataLoader(X, Y, batch_size=batch_size, shuffle=False)\n",
        "    for epoch in range(epochs):\n",
        "      for idx, batch in enumerate(train_batches):\n",
        "        optimizer.zero_grad()\n",
        "        latent_space, reconstructed, preds = model(X)        \n",
        "        branch1_loss = loss_func(preds, Y)\n",
        "        ae_loss = loss_func(reconstructed, X)\n",
        "        loss = config.loss_scalar*ae_loss + branch1_loss\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        wandb.log({'epoch': epoch, 'loss': ae_loss+branch1_loss, 'ae_loss': ae_loss, 'branch1_loss': branch1_loss})\n",
        "        for i in range(config.latent_dim):\n",
        "          wandb.log({str(i+1): latent_space[:, i: i+1]})\n",
        "        latent_space_v, reconstructed_v, preds_v = model(X_v)        \n",
        "        branch1_loss_v = loss_func(preds_v, Y_v)\n",
        "        ae_loss_v = 2*loss_func(reconstructed_v, X_v)\n",
        "        loss_v = ae_loss_v + branch1_loss_v\n",
        "        wandb.log({'v_loss': loss_v, 'v_ae_loss': ae_loss_v, 'v_branch1_loss': branch1_loss_v})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6Oqpw7VtS_R"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count=10000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Multitask_Learning_Sweeps.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}